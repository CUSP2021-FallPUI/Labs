{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Imposing certain limitations on the regression coefficients making the model more resistent to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1. Regularization with artifical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pylab as plt\n",
    "import math\n",
    "import seaborn as sbn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate random data repeating regressors with minor fluctuations\n",
    "M = 20 #size of the training, validation and test samples\n",
    "np.random.seed(1)\n",
    "\n",
    "X = pd.DataFrame(np.random.randn(3*M,3)) #observstions for three random input variables (features)\n",
    "Y = X.sum(axis = 1) + np.random.randn(3*M) #add a target variable as a sum of the three features and some noise\n",
    "data = pd.concat([Y,X]+[X+0.01*np.random.randn(3*M,3) for i in range(4)], axis = 1)  # add four duplicates of the features with minor noise and stack Y and X into a single dataframe\n",
    "data.columns = ['Y'] + ['x%d'%(i+1) for i in range(data.shape[1]-1)]         # name the columns\n",
    "#split into training, test and validation sets\n",
    "dataTrain=data.iloc[:M,:]\n",
    "dataTest=data.iloc[M:2*M,:]\n",
    "dataValid=data.iloc[2*M:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>x11</th>\n",
       "      <th>x12</th>\n",
       "      <th>x13</th>\n",
       "      <th>x14</th>\n",
       "      <th>x15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.788142</td>\n",
       "      <td>1.624345</td>\n",
       "      <td>-0.611756</td>\n",
       "      <td>-0.528172</td>\n",
       "      <td>1.619363</td>\n",
       "      <td>-0.614866</td>\n",
       "      <td>-0.528191</td>\n",
       "      <td>1.622058</td>\n",
       "      <td>-0.595623</td>\n",
       "      <td>-0.531920</td>\n",
       "      <td>1.613616</td>\n",
       "      <td>-0.606805</td>\n",
       "      <td>-0.537692</td>\n",
       "      <td>1.631123</td>\n",
       "      <td>-0.622822</td>\n",
       "      <td>-0.531764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.195552</td>\n",
       "      <td>-1.072969</td>\n",
       "      <td>0.865408</td>\n",
       "      <td>-2.301539</td>\n",
       "      <td>-1.086935</td>\n",
       "      <td>0.856794</td>\n",
       "      <td>-2.294792</td>\n",
       "      <td>-1.080468</td>\n",
       "      <td>0.885954</td>\n",
       "      <td>-2.301005</td>\n",
       "      <td>-1.078150</td>\n",
       "      <td>0.850794</td>\n",
       "      <td>-2.306702</td>\n",
       "      <td>-1.067915</td>\n",
       "      <td>0.877587</td>\n",
       "      <td>-2.320946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.805829</td>\n",
       "      <td>1.744812</td>\n",
       "      <td>-0.761207</td>\n",
       "      <td>0.319039</td>\n",
       "      <td>1.750997</td>\n",
       "      <td>-0.765639</td>\n",
       "      <td>0.337144</td>\n",
       "      <td>1.740020</td>\n",
       "      <td>-0.757705</td>\n",
       "      <td>0.319211</td>\n",
       "      <td>1.748323</td>\n",
       "      <td>-0.761895</td>\n",
       "      <td>0.305561</td>\n",
       "      <td>1.736750</td>\n",
       "      <td>-0.760716</td>\n",
       "      <td>0.313078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.445823</td>\n",
       "      <td>-0.249370</td>\n",
       "      <td>1.462108</td>\n",
       "      <td>-2.060141</td>\n",
       "      <td>-0.262428</td>\n",
       "      <td>1.458658</td>\n",
       "      <td>-2.062449</td>\n",
       "      <td>-0.253662</td>\n",
       "      <td>1.474193</td>\n",
       "      <td>-2.048984</td>\n",
       "      <td>-0.234663</td>\n",
       "      <td>1.465480</td>\n",
       "      <td>-2.050060</td>\n",
       "      <td>-0.240754</td>\n",
       "      <td>1.441244</td>\n",
       "      <td>-2.056523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.316851</td>\n",
       "      <td>-0.322417</td>\n",
       "      <td>-0.384054</td>\n",
       "      <td>1.133769</td>\n",
       "      <td>-0.350348</td>\n",
       "      <td>-0.364679</td>\n",
       "      <td>1.137433</td>\n",
       "      <td>-0.314009</td>\n",
       "      <td>-0.385083</td>\n",
       "      <td>1.145238</td>\n",
       "      <td>-0.314565</td>\n",
       "      <td>-0.390703</td>\n",
       "      <td>1.114319</td>\n",
       "      <td>-0.318158</td>\n",
       "      <td>-0.383564</td>\n",
       "      <td>1.144792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Y        x1        x2        x3        x4        x5        x6  \\\n",
       "0 -0.788142  1.624345 -0.611756 -0.528172  1.619363 -0.614866 -0.528191   \n",
       "1 -2.195552 -1.072969  0.865408 -2.301539 -1.086935  0.856794 -2.294792   \n",
       "2  1.805829  1.744812 -0.761207  0.319039  1.750997 -0.765639  0.337144   \n",
       "3  0.445823 -0.249370  1.462108 -2.060141 -0.262428  1.458658 -2.062449   \n",
       "4  0.316851 -0.322417 -0.384054  1.133769 -0.350348 -0.364679  1.137433   \n",
       "\n",
       "         x7        x8        x9       x10       x11       x12       x13  \\\n",
       "0  1.622058 -0.595623 -0.531920  1.613616 -0.606805 -0.537692  1.631123   \n",
       "1 -1.080468  0.885954 -2.301005 -1.078150  0.850794 -2.306702 -1.067915   \n",
       "2  1.740020 -0.757705  0.319211  1.748323 -0.761895  0.305561  1.736750   \n",
       "3 -0.253662  1.474193 -2.048984 -0.234663  1.465480 -2.050060 -0.240754   \n",
       "4 -0.314009 -0.385083  1.145238 -0.314565 -0.390703  1.114319 -0.318158   \n",
       "\n",
       "        x14       x15  \n",
       "0 -0.622822 -0.531764  \n",
       "1  0.877587 -2.320946  \n",
       "2 -0.760716  0.313078  \n",
       "3  1.441244 -2.056523  \n",
       "4 -0.383564  1.144792  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataTrain.head() #training data structure (15 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.769\n",
      "Model:                            OLS   Adj. R-squared:                  0.726\n",
      "Method:                 Least Squares   F-statistic:                     17.79\n",
      "Date:                Tue, 19 Oct 2021   Prob (F-statistic):           2.38e-05\n",
      "Time:                        15:12:44   Log-Likelihood:                -23.431\n",
      "No. Observations:                  20   AIC:                             54.86\n",
      "Df Residuals:                      16   BIC:                             58.84\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.3465      0.197      1.757      0.098      -0.072       0.765\n",
      "x1             0.9706      0.222      4.369      0.000       0.500       1.441\n",
      "x2             1.2626      0.271      4.666      0.000       0.689       1.836\n",
      "x3             0.9113      0.181      5.026      0.000       0.527       1.296\n",
      "==============================================================================\n",
      "Omnibus:                        2.447   Durbin-Watson:                   1.363\n",
      "Prob(Omnibus):                  0.294   Jarque-Bera (JB):                1.060\n",
      "Skew:                          -0.521   Prob(JB):                        0.589\n",
      "Kurtosis:                       3.433   Cond. No.                         1.63\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#perform a regression of the target variable against those three features we used to compute it\n",
    "lm = smf.ols(formula = 'Y ~ x1+x2+x3', data = dataTrain).fit()\n",
    "OLS_coef=lm.params\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 is somewhat high and the coefficient estimates are relatively close to the original ones we used - confidence intervals include 1; some deviations might be attributed to adding substantial noise to the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we do better if we add other features? Although we know they are random and target variable has little to do with them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.933\n",
      "Model:                            OLS   Adj. R-squared:                  0.682\n",
      "Method:                 Least Squares   F-statistic:                     3.715\n",
      "Date:                Tue, 19 Oct 2021   Prob (F-statistic):              0.107\n",
      "Time:                        15:12:44   Log-Likelihood:                -11.065\n",
      "No. Observations:                  20   AIC:                             54.13\n",
      "Df Residuals:                       4   BIC:                             70.06\n",
      "Df Model:                          15                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.1937      0.608      0.318      0.766      -1.495       1.883\n",
      "x1            -9.2631     85.921     -0.108      0.919    -247.819     229.293\n",
      "x2           -32.4096    202.204     -0.160      0.880    -593.818     528.999\n",
      "x3          -134.6252    172.665     -0.780      0.479    -614.019     344.769\n",
      "x4           -11.1610     34.612     -0.322      0.763    -107.259      84.938\n",
      "x5           -21.1312     59.100     -0.358      0.739    -185.219     142.956\n",
      "x6           102.0380     81.195      1.257      0.277    -123.396     327.472\n",
      "x7           -25.4076     33.632     -0.755      0.492    -118.785      67.970\n",
      "x8             4.8265     67.778      0.071      0.947    -183.354     193.007\n",
      "x9           -40.7081     37.400     -1.088      0.338    -144.546      63.130\n",
      "x10           12.9814     35.845      0.362      0.736     -86.541     112.504\n",
      "x11           64.2255     59.005      1.088      0.338     -99.597     228.048\n",
      "x12           36.4580     50.701      0.719      0.512    -104.310     177.226\n",
      "x13           33.2591     64.390      0.517      0.633    -145.517     212.035\n",
      "x14          -14.2926     72.576     -0.197      0.853    -215.795     187.210\n",
      "x15           37.9777     44.690      0.850      0.443     -86.101     162.057\n",
      "==============================================================================\n",
      "Omnibus:                        3.907   Durbin-Watson:                   2.279\n",
      "Prob(Omnibus):                  0.142   Jarque-Bera (JB):                3.040\n",
      "Skew:                          -0.943   Prob(JB):                        0.219\n",
      "Kurtosis:                       2.700   Cond. No.                     3.39e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.39e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#now perform the regression over all the 15 features including noisy duplicates\n",
    "#use \"join\" to engineer a string regression formulae, stacking together feature names adding '+' in between\n",
    "lm2 = smf.ols(formula = 'Y ~ '+ '+'.join(dataTrain.columns[1:]), data = dataTrain).fit()\n",
    "OLS_coef=lm2.params\n",
    "print(lm2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 increased, but neither p-value nor coefficient estimates make much sense now.\n",
    "\n",
    "Perfect case of multicollinearity and likely overfitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8927084960997744"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#indeed the test R2 is negative this way\n",
    "r2_score(dataTest.Y,lm2.predict(dataTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we see from the regression itself that smth is wrong? Let's look at the coefficients having pretty hight magnitude compared to the original scale of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept      0.193676\n",
       "x1            -9.263060\n",
       "x2           -32.409580\n",
       "x3          -134.625216\n",
       "x4           -11.160992\n",
       "x5           -21.131152\n",
       "x6           102.037978\n",
       "x7           -25.407631\n",
       "x8             4.826498\n",
       "x9           -40.708135\n",
       "x10           12.981378\n",
       "x11           64.225496\n",
       "x12           36.458027\n",
       "x13           33.259082\n",
       "x14          -14.292609\n",
       "x15           37.977691\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea of regularization\n",
    "\n",
    "When fitting a linear regression with multiple features $x=(x^1,x^2,...,x^n)$ \n",
    "$$\n",
    "y\\sim w^T x\n",
    "$$\n",
    "the model might become complex and susceptible to overfitting. Complexity often comes with coefficients $w$ growing large in absolute values.\n",
    "\n",
    "One way to reduce complexity is to penalize regression for the magnitude of the coefficients $w=(w^1,w^2,...,w^n)$, which can be measured by\n",
    "$$\n",
    "||w||_1=\\sum\\limits_j |w^j|\n",
    "$$\n",
    "or by \n",
    "$$\n",
    "||w||_2^2=\\sum\\limits_j (w^j)^2\n",
    "$$\n",
    "So instead of simply minimizing \n",
    "$$\n",
    "RSS(w)=\\sum\\limits_j (y_j-w^T x_j)^2\n",
    "$$\n",
    "where $x_j, y_j$ are observations for regressors and output variable, we minimize\n",
    "$$\n",
    "RSS(w)+\\lambda ||w||_2^2=RSS(w)+\\lambda \\sum\\limits_j (w^j)^2\\to\\min\\hspace{10ex}(Ridge)\n",
    "$$\n",
    "or\n",
    "$$\n",
    "RSS(w)+\\lambda ||w||_1=RSS(w)+\\lambda \\sum\\limits_j \\left|w^j\\right|\\to\\min\\hspace{10ex}(Lasso)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model is known as Ridge, the second - as Lasso (least absolute shrinkage and selection operator) regularized regression.\n",
    "\n",
    "Both Ridge and Lasso could be shown to be equivalent to a constrained minimization of $RSS$:\n",
    "\n",
    "$$\n",
    "RSS(w)\\to min, \\ ||w||_p\\leq \\alpha,\n",
    "$$\n",
    "\n",
    "with $p=1,2$ respectively, although analytic relation between constants $\\alpha$ and $\\lambda$ is somewhat nontrivial. In practice however the choice of $\\lambda$ or $\\alpha$ is usually empirical anyway, so both regularized or constrained forms of the optimization problem are equally applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization problems (Ridge) and (Lasso) tend to minimize $RSS$ at the same time penalizing the regression for having $||w||$ too large (regularization) which often leads to the model complexity through multiple regressors with large coefficients canceling effect of each other. So in a sense Lasso and Ridge are trying to avoid this situation, looking for relatively simple \"regular\" models with best possible fit. \n",
    "\n",
    "** NOTICE ** As the order of magnitude of $w^j$ is directly related to the scale of the regressors, it is practical to rescale them (e.g. by standardizing) to make sure the $w^j$ are comparable in scale. Otherwise penalization terms directly mixing components $w^j$ of different, sometimes incomparable, scale do not make too much sense.\n",
    "\n",
    "From Bayesian standpoint (for those familiar with Bayesian inference) Lasso and Ridge simply perform the regression with the prior belief that all the components of the $w$ are limited through the fixed variance of the priors. Such a belief affects the final outcome of the model making solutions with large $||w||$ to be particularly unlikely.\n",
    "\n",
    "This helps Ridge and Lasso to fight overfitting also dealing with multicollinearity of regressors to some extent, preventing from learning noise through particularly complex \"unnatural\" combinations of the regressors.\n",
    "\n",
    "Ridge regression admits solution in the closed form (consider partial derivatives of the objective function with respect to $w_j$):\n",
    "\n",
    "$$\n",
    "\\hat{w}=(X^T X+\\lambda I)^{-1}X^T Y, \\hspace{5ex}(Ridge\\ solution)\n",
    "$$\n",
    "\n",
    "where $I$ is the identity $n\\times n$ matrix, while $n$ being the number of regressors. The formulae (Ridge solution) shows that the Ridge regression can in theory deal with the case of multicollinearity, when the matrix $X^T X$ is singular and OLS estimate does not exist.\n",
    "\n",
    "Lasso does not admit solution in the closed form and requires numerical methods (like subgradient methods) to be fit. \n",
    "Lasso however has an advantage of being often able to completely eliminate impact of certain irrelevant regressors setting the corresponding slope coefficients to zero.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and cross-validation\n",
    "\n",
    "A good way to evaluate the model is to use a test set separate from the training sample to estimate the model performance. Additionally if the model depends on the certain parameters (like $\\alpha,\\lambda$ for Lasso/Ridge) which are not supposed to be fit during the training phase, a separate validation sample could be used for the selection of model parameters (we pick up those which optimize model performance over the validation set). Usually test and validation sets are got as subsamples (often random) of the available dataset, while the remaining data is used as the training sample. \n",
    "\n",
    "But often the available dataset is small enough, so splitting into into even smaller traning, validation and test sets could have negative impact on the model training leading to noisy and unreliable models. In such cases cross-validation is often applied, performing not one but several random splits of the sample with further averaging of the model performance scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of $\\lambda$ or $\\alpha$. Model validation.\n",
    "\n",
    "In both - constrained and regularized forms - the parameter $\\alpha$ or $\\lambda$ of Lasso/Ridge regression is somewhat arbitrary and can take any value from $0$ to $+\\infty$. Small values of $\\lambda$ or high values of $\\alpha$ lead to the result close to OLS (identical to it is $\\lambda=0$ or $\\alpha=+\\infty$), while large $\\lambda$ or small $\\alpha$ tend to overemphasize the impact of regularization over the fit itself. There is no single best way of choosing the value of the regularization parameter - what is usually suggested is to fit it by evaluating the model for different values of the parameter over the separate validation set and picking up the value for which the validation performance is the best one. As for the performance metric one can use RSS or, equivalently, R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IS R-squared of Ridge is: 0.769666378885421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.1896624 , 0.24722858, 0.18132605, 0.21763589, 0.24170847,\n",
       "       0.17970417, 0.15691973, 0.22943271, 0.11503989, 0.19477322,\n",
       "       0.28970272, 0.23323684, 0.19730613, 0.2294736 , 0.19280885])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try Ridge with an arbitrary regularization parameter alpha=1\n",
    "Ridge=linear_model.Ridge(fit_intercept=True, alpha=1)\n",
    "Ridge.fit(dataTrain.iloc[:,1:],dataTrain.Y)\n",
    "# In sample:\n",
    "YPred_IS=Ridge.predict(dataTrain.iloc[:,1:])\n",
    "print(\"The IS R-squared of Ridge is: {0}\".format(r2_score(dataTrain.Y,YPred_IS)))\n",
    "Ridge.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OS R-squared of Ridge is: 0.3431169851850969\n"
     ]
    }
   ],
   "source": [
    "#Out of sample\n",
    "YPred_OS=Ridge.predict(dataTest.iloc[:,1:])\n",
    "print(\"The OS R-squared of Ridge is: {0}\".format(r2_score(dataTest.Y,YPred_OS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IS R-squared of Lasso is: 0.7545180291370721\n",
      "The OS R-squared of Lasso is: 0.41632294784987245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.80668997, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.02567708, 0.81067886, 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try Lasso with an another arbitrary regularization parameter alpha=0.01 (if we try alpha=1 this turns out to be too much regularization already)\n",
    "Lasso=linear_model.Lasso(fit_intercept=True, alpha=0.1)\n",
    "\n",
    "Lasso.fit(dataTrain.iloc[:,1:],dataTrain.Y)\n",
    "# In sample:\n",
    "YPred_IS=Lasso.predict(dataTrain.iloc[:,1:])\n",
    "print(\"The IS R-squared of Lasso is: {0}\".format(r2_score(dataTrain.Y,YPred_IS)))\n",
    "#Out of sample\n",
    "YPred_OS=Lasso.predict(dataTest.iloc[:,1:])\n",
    "print(\"The OS R-squared of Lasso is: {0}\".format(r2_score(dataTest.Y,YPred_OS)))\n",
    "Lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the Alpha and report best test performance for Ridge/Lasso.\n",
    "def Regularization_fit_alpha(model,X_train,y_train,X_valid,y_valid,alphas,Graph=False, logl=False):\n",
    "    #model = 1-Ridge, 2-Lasso\n",
    "    #aplhas: a list of aplha values to try\n",
    "    #Graph: plot the graph of R^2 values for different alpha\n",
    "\n",
    "    R_2_OS=[] #out of sample R2's\n",
    "    \n",
    "    #initialize the model to use for each given alpha\n",
    "    if model==1:\n",
    "        RM = lambda a: linear_model.Ridge(fit_intercept=True, alpha=a)\n",
    "        model_label='Ridge'\n",
    "    else:\n",
    "        RM = lambda a: linear_model.Lasso(fit_intercept=True, alpha=a)\n",
    "        model_label='Lasso'\n",
    "    \n",
    "    best_R2 = -1\n",
    "    best_alpha = alphas[0]\n",
    "    best_coefs = []\n",
    "    \n",
    "    for a in alphas: #for all alphas to try\n",
    "        lm = RM(a) #assign the model\n",
    "        lm.fit(X_train,y_train)  #fit the regularization model\n",
    "        y_predict=lm.predict(X_valid) #compute the prediction for the validation sample \n",
    "        R_2_OS_=r2_score(y_valid,y_predict)\n",
    "        R_2_OS.append(R_2_OS_)\n",
    "        if R_2_OS_ > best_R2: #if current OS R2 is the best - update the best solution\n",
    "            best_R2 = R_2_OS_\n",
    "            best_alpha = a\n",
    "            best_coefs = lm.coef_\n",
    "    \n",
    "    if Graph==True: #visualization of the performance depending on alpha\n",
    "        plt.title('OS-R-squared for different Alpha')\n",
    "        if logl: #log-scale\n",
    "            plt.xlabel('ln(Alpha)')\n",
    "            l=np.log(alphas)\n",
    "            bl=np.log(best_alpha)\n",
    "        else:\n",
    "            plt.xlabel('Alpha')\n",
    "            l=alphas\n",
    "            bl=best_alpha\n",
    "        plt.ylim((-0.1,1))\n",
    "        plt.plot(l,R_2_OS,'b',label=model_label)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.ylabel('R-squared')\n",
    "        plt.axvline(bl,color='r',linestyle='--')\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    return [best_alpha,best_R2,best_coefs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAsnklEQVR4nO3de7wVZdn/8c/FBoQtAnHwBCiYoqABGqBiKSUi4inyiGmiJk/5kGJaHlLzVFqpKWYe8pSaGqH1w3OpqWWQbhQR4dEMUTaIAh4QARG5fn9cs2Ox3YcF7Nmz9p7v+/W6X7PWzKyZa+3DXDP3fc895u6IiEh+tcg6ABERyZYSgYhIzikRiIjknBKBiEjOKRGIiOScEoGISM4pEUjumZmb2fa1LNvCzJ4xs4/M7MoU9t0z2X/L5P0jZnZ8wfJLzWyxmS1M3o8ys3lmtszMdm3oeNJmZnPNbFhDrysbR4mgGTGzMWb2spktN7OFZna9mXUsWN7RzG5Nln1kZq+Z2dl1bO92M1uVHHTeM7O/mtlOjfJlSsdYYDHQ3t3PSHtn7n6Au/8OwMy2Ac4A+rr7lskqVwDj3L2du7+YdjyFzOwpM/tOEeu1S/5mHmmMuGTjKRE0E2Z2BvBz4IdAB2APYFvgr2bWOlntV0A7oE+yziHA6/Vs+hfu3g7oBswHbmn46BtO1Zl1A9oWmOUbcOdlA8SyDbDE3d+tFs8rG7KxFH42tTkM+ATYz8y2rG9lKQHurtLEC9AeWAYcWW1+O2ARcGLyfibwjfXY7u3ApQXvRwIf17H+YKACWAq8A1xVsOw44E1gCfBjYC4wrJb9DAUqC96fDfwH+AiYBYwqWDYGeJZIckuAS4FNiDPnt5I4bgDaFnzmh8DbwALgRMCB7Wv5/p8Cq5Kf77Bk21cnn12QvN6kMG7gLGAhcGcN2yxLYlsMzAH+N9l/y2T5U8B3kn2tANYk+74nmTrwMfCfZP2tgfuS3/MbwKkF+7oQmATclfxOvkOcANySfP/5yc+rrOBn+Y8kvveT7R2QLPsp8BmwMonj13X8HTyZrP8CcGa1ZYW/96r4/pD8bl8A+ldb90xgBvBhsl6bZNkXgAeT7/1+8rp71v+LTbXoiqB5GAK0Ae4vnOnuy4CHgf2SWVOBn5rZCWa2w/rswMw2BUZT9xXENcA17t4e+CIwMflsX+B6IhlsDXQGuq/H7v8DfJU4iF0E3GVmWxUs3504qG5BHIAuB3oDA4DtiauZC5JYRhAHl/2AHYgDbo3cfQzwe5KrInd/nEhieyTb7k8kv/MKPrYl0Ik4cx9bw2ZPBg4CdgUGAofXsu/HgQOABcm+R3tcmUEcLL9oZi2AB4CXku+4LzDezPYv2NShxMG2Y/JdbgdWJz+XXYHhRIKosjvwKtAF+AVwi5mZu/8Y+Dtrq6XG1RS3mW1LJMTfJ+XbNa1XLb4/Ej+zu4E/m1mrguVHAiOAXkA/IllB1GbcRvyctyGS5q/r2ZfUQomgeegCLHb31TUseztZDvB94p9zHDDLzF43swPq2faZZvYBccb2FeJgXptPge3NrIu7L3P3qcn8w4EH3f0Zd/8EOJ840y2Ku//R3Re4+xp3/wPwb+IAXGWBu1+bfP+VxAH4dHd/z90/An4GHJ2seyRwm7vPdPePibPS9fEt4GJ3f9fdFxGJqfBnsgb4ibt/4u4ravj8kcDV7j7P3d8DLlvP/RcaBHR194vdfZW7zwF+y9rvCjDF3f/s7muIK8eRwHh3/9ijyulX1dZ/091/6+6fAb8DtiISbLGOA2a4+yzgXmDnehq1p7n7JHf/FLiKOKHZo2D5hOR3/x6R9AYAuPsSd7/P3Zcnv+OfAvusR5xSQImgeVgMdKmlDnirZDnuvsLdf+buXybOyicCfzSzTmZ2btLAt8zMbij4/BXu3hHoSZx17QhgZt8qWL+qUfAk4kz8/8zseTM7KJm/NTCvaoPJAXhJsV/OzL5tZtPN7IMkKe3C2uRG4baBrkA5MK1g/UeT+Z+LhaiuWh9bV/vMm8m8KovcfWU9n9+Y/RfaFti66nsm3/Vc1j1wz6u2fivg7YL1bwQ2L1hnYdULd1+evGxH8b5NnGzg7vOBp4Hj61i/8O9iDVG1VvjzXFjwenlVLGZWbmY3mtmbZrYUeAboaGZl6xGrJJQImocpROPcNwtnmlk7onrhieofcPelxJnypkCvJEG0S8p3a1j/LeA04Boza+vuvy9Y/4BknX+7+2jiwPJzYFJSpfQ20KMgrnIiEVX5mDh4V9myYN1tibPccUDnJCnNBKwwvILXi4mEtbO7d0xKh4JqlXViIaoV1scC4oBa+PkFtcRSk43df6F5wBsF37Oju2/m7iNriWce8XfSpWD99u6+c5H7q/O7mdkQorrtnKRn2kKiqumYOhqqC/8uWhBVhgtqWbfQGcRJye5JVeTeVZsp4rNSjRJBM+DuHxJVFNea2Qgza2VmPYkz/krgTgAzO9/MBplZazNrQxzYPyDqhIvZz1+Jf9Ka6r4xs2PNrGtyZvdBMnsNUUd9kJl9JenBdDHr/u1NB0YmVyZbAuMLlm1KHIAWJfs4gbgiqC3GNUTi+JWZbZ58pltBvflEYIyZ9U0S0k+K+e4F7gHOM7OuZtaFaHu4az0+PxE41cy6m9kXiIbwDfUc8JGZnWVmbc2szMx2MbNBNa3s7m8DfwGuNLP2ZtbCzL5oZsVWqbwDbFfH8uOBvwJ9iSqcAcTvqi1xQlKTL5vZN5NEMZ5IVFNrWbfQZkTC/8DMOrH+v0cpoETQTLj7L4hqgSuIHiL/Is4A903q5SEOqLcRZ80LiAbTA5NG5WL9EviRmW1Sw7IRwCtmtoxoOD46qY56hegdczdxRvw+kaCq3Ek0eM4lDlR/KPhes4Ariaued4AvEb2E6nIW0ag9Nak2eJykSsvdHyF6+jyZrPNkkd+7yqVEz6gZwMtET5dL1+PzvwUeI77vC1Rr4F8fST3+QcQB9w3i93oz0ahem28DrYneV+8TSXqrOtYvdA1wuJm9b2YTChckJxZHAte6+8KC8gbx+62teuj/AUclsRwHfDNpL6jP1USCWUwkjkeL/A5SA3PXg2mk8ZnZXOA7Se8YySEzu5Dotnts1rHkna4IRERyLrVEkAxl8K6ZzaxluZnZhKQL4wwz2y2tWEREpHapVQ2Z2d7EHYh3uPvnGvfMbCTRr30k0bPgGnffPZVgRESkVqldEbj7M8B7daxyKJEkPLnxqGO1u0VFRKQRNNYgVDXpxro3u1Qm896uvqKZjSXpsrjpppt+eaed8jYApuTSq0mv3h13zDYOaRamTZu22N271rQsy0RQNHe/CbgJYODAgV5RUZFxRCKNYOjQmD71VJZRSDNhZrXexZ5lr6H5rHuHZfdknoiINKIsrwgmA+PM7F6isfjD5M5HEQE477z61xFpAKklAjO7hxiOtouZVRK3gLcCcPcbiOGRRxJ3dy4HTkgrFpEmaZie0iiNI7VEkAw+VtdyJ4YdEJGaTJ8e0wEDsoyiyfn000+prKxk5cq6BoFtvtq0aUP37t1p1apV/SsnmkRjsUgujR8fUzUWr5fKyko222wzevbsiVm+BiN1d5YsWUJlZSW9evUq+nMaYkJEmpWVK1fSuXPn3CUBADOjc+fO6301pEQgIs1OHpNAlQ357koEIiI5p0QgItLAysrKGDBgALvssgsHH3wwH3zwAQALFizg8MMPr/EzQ4cOJaubZZUIRErVz34WRZqctm3bMn36dGbOnEmnTp247rrrANh6662ZNGlSxtF9nhKBSKkaMiSKNGl77rkn8+fHoAlz585ll11iMOYVK1Zw9NFH06dPH0aNGsWKFSv++5lbbrmF3r17M3jwYE4++WTGjRsHwKJFizjssMMYNGgQgwYN4tln63tYX3HUfVSkVP3znzFVMthg48evvR2joQwYAFdfXdy6n332GU888QQnnXTS55Zdf/31lJeXM3v2bGbMmMFuu8UjWRYsWMAll1zCCy+8wGabbcbXv/51+vfvD8Bpp53G6aefzle+8hXeeust9t9/f2bPnr3R30mJQKRUnXtuTHUfQZOzYsUKBgwYwPz58+nTpw/77bff59Z55plnOPXUUwHo168f/fr1A+C5555jn332oVOnTgAcccQRvPbaawA8/vjjzJo167/bWLp0KcuWLaNdu3YbFa8SgYg0W8WeuTe0qjaC5cuXs//++3Pdddf996C/MdasWcPUqVNp06ZNA0S5ltoIRERSUl5ezoQJE7jyyitZvXr1Osv23ntv7r77bgBmzpzJjBkzABg0aBBPP/0077//PqtXr+a+++7772eGDx/Otdde+9/30xuo3kuJQEQkRbvuuiv9+vXjnnvuWWf+9773PZYtW0afPn244IIL+PKXvwxAt27dOPfccxk8eDB77bUXPXv2pEOHDgBMmDCBiooK+vXrR9++fbnhhhsaJMbUnlmcFj2YRnJDD6bZILNnz6ZPnz5Zh7FRqur9V69ezahRozjxxBMZNWpU0Z+v6WdgZtPcfWBN66uNQKRUZVXBLZm78MILefzxx1m5ciXDhw/nG9/4Rqr7UyIQKVUafjq3rrjiikbdn9oIRErV449HkfXW1Kq8G9KGfHddEYiUqksvjameVLZe2rRpw5IlS3I5FHXV8wjWt3upEoGINCvdu3ensrKSRYsWZR1KJqqeULY+lAhEpFlp1arVej2dS9RGICKSe0oEIiI5p6ohkVJ1441ZRyA5oUQgUqp23DHrCCQnVDUkUqoeeCCKSMp0RSBSqq68MqYHH5xtHNLs6YpARCTnlAhERHJOiUBEJOeUCEREck6NxSKl6s47s45AckKJQKRU9eiRdQSSE6lWDZnZCDN71cxeN7Oza1i+jZn9zcxeNLMZZjYyzXhEmpQ//CGKSMpSSwRmVgZcBxwA9AVGm1nfaqudB0x0912Bo4HfpBWPSJNz/fVRRFKW5hXBYOB1d5/j7quAe4FDq63jQPvkdQdgQYrxiIhIDdJMBN2AeQXvK5N5hS4EjjWzSuBh4Ps1bcjMxppZhZlV5PVhEyIiacm6++ho4HZ37w6MBO40s8/F5O43uftAdx/YtWvXRg9SRKQ5SzMRzAcKuz10T+YVOgmYCODuU4A2QJcUYxIRkWrS7D76PLCDmfUiEsDRwDHV1nkL2Be43cz6EIlAdT8iAJMmZR2B5ERqicDdV5vZOOAxoAy41d1fMbOLgQp3nwycAfzWzE4nGo7HuLunFZNIk9JFF8fSOFK9oczdHyYagQvnXVDwehawV5oxiDRZt98e0zFjsoxCciDrxmIRqc3tt69NBiIpUiIQEck5JQIRkZxTIhARyTklAhGRnNMw1CKl6uGH619HpAEoEYiUqvLyrCOQnFDVkEip+s1vooikTIlApFRNnBhFJGVKBCIiOadEICKSc0oEIiI5p0QgIpJz6j4qUqqeeirrCCQndEUgIpJzSgQipeqKK6KIpEyJQKRUPfhgFJGUKRGIiOScEoGISM4pEYiI5Jy6j4qUqrZts45AckKJQKRUPfJI1hFITqhqSEQk55QIRErVJZdEEUmZEoFIqXriiSgiKVMiEBHJOSUCEZGcUyIQEck5dR8VKVWdO2cdgeSEEoFIqbrvvqwjkJxItWrIzEaY2atm9rqZnV3LOkea2Swze8XM7k4zHhER+bzUrgjMrAy4DtgPqASeN7PJ7j6rYJ0dgHOAvdz9fTPbPK14RJqcc86J6WWXZRuHNHtpVg0NBl539zkAZnYvcCgwq2Cdk4Hr3P19AHd/N8V4RJqWKVOyjkByIs2qoW7AvIL3lcm8Qr2B3mb2rJlNNbMRNW3IzMaaWYWZVSxatCilcEVE8inr7qMtgR2AocBo4Ldm1rH6Su5+k7sPdPeBXbt2bdwIRUSauTQTwXygR8H77sm8QpXAZHf/1N3fAF4jEoOIiDSSNBPB88AOZtbLzFoDRwOTq63zZ+JqADPrQlQVzUkxJpGmo3v3KCIpS62x2N1Xm9k44DGgDLjV3V8xs4uBCnefnCwbbmazgM+AH7r7krRiEmlS7ror6wgkJ8zds45hvQwcONArKiqyDkNEpEkxs2nuPrCmZVk3FotIbcaPjyKSMg0xIVKqpk/POgLJiToTgZk9ANRad+TuhzR4RCIi0qjquyK4Ipl+E9gSqGq9Gg28k1ZQIiLSeOpMBO7+NICZXVmtkeEBM1OLrYhIM1BsG8GmZrZdwbhBvYBN0wtLROjdO+sIJCeKTQSnA0+Z2RzAgG2B/0ktKhGBm27KOgLJiaISgbs/mgwZvVMy6//c/ZP0whIRkcZS1H0EZlYO/BAY5+4vAduY2UGpRiaSd2PHRhFJWbE3lN0GrAL2TN7PBy5NJSIRCa+9FkUkZcUmgi+6+y+ATwHcfTnRViAiIk1csYlglZm1Jbm5zMy+CKiNQESkGSi219BPgEeBHmb2e2AvYExaQYmISOOpNxGYWQvgC8TdxXsQVUKnufvilGMTybcBA7KOQHKi3kTg7mvM7EfuPhF4qBFiEhGAq6/OOgLJiWLbCB43szPNrIeZdaoqqUYmIiKNotg2gqOS6f8WzHNgu4YNR0T+69hjY6onlUnKir2zuFfagYhINZWVWUcgOVH0g2nMbBegL9Cmap6735FGUCIi0niKSgRm9hNgKJEIHgYOAP4BKBGIiDRxxTYWHw7sCyx09xOA/kCH1KISEZFGU2zV0IqkG+lqM2sPvAv0SDEuEdlzz/rXEWkAxSaCCjPrCPwWmAYsA6akFZSIAJddlnUEkhPF9ho6JXl5g5k9CrR39xnphSUiIo2l2MbivWua5+7PNHxIIgLAYYfF9L77so1Dmr1iq4Z+WPC6DTCYqCL6eoNHJCJhyZKsI5CcKLZq6ODC92bWA7g6jYBERKRxFdt9tLpKoE9DBiIiItkoto3gWpKH0hDJYwDwQkoxiYhIIyq6+2jB69XAPe7+bArxiEiVfffNOgLJiWLbCH6XdiAiUs3552cdgeREsVVDL7O2amidRYC7e79aPjcCuAYoA25298trWe8wYBIwyN0ralpHJA+WL4eXX4b58+GDD6BVK+jSBXr3hl69oMWGtuqJ1KHYqqFHkumdyfRbyfT62j5gZmXAdcB+ROPy82Y22d1nVVtvM+A04F/FBi3SnMyZA/feC3/6E7z4Inz2Wcx/mAMAGJn8+22+OYwYAccfD1/7GphlFbE0N8WeX+zn7j9y95eTcjYw3N3fdPc3a/nMYOB1d5/j7quAe4FDa1jvEuDnwMr1jl6kiXKHp5+GAw+EL34RfvxjaNkSzjoL/vznSAhf230FQwev4O9/h5tuiiaDBx6I6c47w/33x3ZENlaxicDMbK+CN0OK+Gw3YF7B+8pkXuFGdwN6uHudz0I2s7FmVmFmFYsWLSoyZJHSNGNGHMyHDoXnn4eLLoK5c2HKFPjpT+HQQ+O59W3aQNu28JWvwMknw913R5XR75IWu8MOg332gX//O8MvI81CsYngJOA3ZjbXzN4EfgOcuDE7NrMWwFXAGfWt6+43uftAdx/YtWvXjdmtSGaWLoVTToFdd41kMGECvPkmXHABbLttcdto2xa+/e34/I03RntC//5www26OpANV1QicPdp7t6feA5BP3cf4O713Ucwn3WHqu6ezKuyGbAL8JSZzQX2ACab2cBigxdpKp54Ar70pTh4jxsXZ/Hf/34c2DdEy5YwdizMnAlf/Sp873tx1fDJJw0bt+RDUYnAzE5LnkOwFLjSzF4ws+H1fOx5YAcz62VmrYGjgclVC939Q3fv4u493b0nMBU4RL2GpDn59FM47TQYNiwO+v/8J1xzDXzhC0V8+KCDotShWzd45JFoY7jlFthvP/jww4aJXfKj2KqhE919KTAc6AwcB9TYFbSKu68GxgGPAbOBie7+ipldbGaHbETMIk3CkiUwfHhUAZ12WjQA7777emzgzDOj1KNFC7j00mhDmDIl2h8WL97wuCV/iu0+WtVRbSRwR3JAr7fzmrs/TDzjuHDeBbWsO7TIWERK3iuvwCGHROPuXXfBt75V/2c21ujR0KFDNCIPHRq9kjp3Tn+/0vQVe0Uwzcz+QiSCx5K+/2vSC0uk6XrwwXjK5PLl8MwzG5EEhg6Nsh5GjoSHHoLXX4/XH320gfuWXFmfXkNnE3f+LgdaAyekFpVIE+QOv/xlXAn07h1dQwcPbvw4vv51mDgRpk2DUaPUgCz1K7bX0Bp3f8HdPzCzC919iR5VKbLWypVxx++PfgRHHhlXAt27ZxfPIYfArbdGb6WxY9W1VOq2ISOXqKFXpMDChTHkw513wiWXwD33QHl51lHF/QYXXQR33AFXXJF1NFLKim0sLqQRTkQSL74YZ9/vvRePFv7mN7OOaF3nnx8N12edBX361NsbVXJqQ64IvmxmLcysEfpBiJSuSZNgr71i8Ldnn00hCRx5ZJSNYAa33RZ3Mx9zjIajkJrVmQjMrL2ZnWNmvzaz4UmX0VOAOcDG/YWKNFFr1kSVyxFHxAH2+edjbKAGd8opUTZSeXkMZNe6dcS8YsXGhybNS31XBHcCOwIvA98B/gYcAXzD3WsaSVSkWfv4YzjqKLjwQhgzBp58ErbYIqWdLV8epQH06BFtGC+9BOPHN8gmpRmpr41gO3f/EoCZ3Qy8DWzj7hoyWnJn3rwYGXT69Gh8/cEPUn4mwMiRMX3qqQbZ3AEHwNlnw+WXx6ilxxzTIJuVZqC+K4JPq164+2dApZKA5NGUKTBoEPznP3HD2BlnNM0Hw1xySQxSN3as2gtkrfoSQX8zW5qUj4B+Va/NbGljBCiStVtuiRt827WLhFB1ot4UtWwZYxK1bg3HHhuD4onUmQjcvczd2ydlM3dvWfC6fWMFKZKFTz+NoaK/852oSnnuOejbN+uoNl737jEc9nPPxYNwRPQobJEaLFoUI4f++tdRDfTww9CpU9ZRNZwjjoDjjotRS6dOzToaydqG3FAm0qz961/RM2jhwuhpc+yxGQUyZkyqm7/22hgK49hjowG8XbtUdyclTFcEIgl3uOqqeEYwwD/+kWESgEgEKSaDDh0i0c2Zoy6leadEIEIMEXHooVENdNBBMXTEwKwfmrp4cepPmPnqV2P4iVtugcmT619fmiclAsm9J5+MO4MffTQeI3n//UU+SjJthx8eJWUXXQT9+8czjxctSn13UoKUCCS3li+HU0+NRzu2aRPjBZ16atO8P2BjtG4dVUQffAD/8z8asjqPlAgkl6ZMiauAa6+Ng//06XHDWF596UvRg+hPf4qkIPmiRCC58t578N3vxqihq1bFg1uuuaY0nh+QtR/8INoMvv99eOutrKORxqREILngDr/7Hey0E9x8c/SSefnleKyjhLIyuP32GF11zJiYSj4oEUiz9/e/w5AhcXDbfvt4lu9VV8Fmm2UdWT2+970ojWi77eBXv4K//Q0mTGjUXUuGlAik2Zo1K54etvfeUdVx661xb0D//llHVqSjjorSyE46KbrQnn12/Ayl+VMikGZnxgwYPToaQJ9+Gi67LEbaPOEEaNGU/uLnzYvSyMzgt7+NO42//W0NTJcHTenfQqRW7tH986CD4oz/oYfgzDPjrtmzz26ijcHHHRclA1tuGQPTTZsWvYmkeVMikCbt44/j7HW33WJoiKlTY8z9N9+En/8cOnfOOsKm67DDIg/99KcxUqk0X0oE0uS4xwF/3DjYeut4yMqaNXD99ZEAzjuvRO4MbgYmTICttoqE0EBPzZQSpNFHpUlwj4bLe++NB6vMmQObbBJnraecEr2C8nZHcGPo2DG6lA4bFlVs6knUPCkRSMn65JNo7H3wwajznzMnGnuHDYMLLoBvfCNG0JR07btv3H09YUL0who2LOuIpKGZN7GBRQYOHOgVFRVZhyEp+OQTqKiIMfKffjq6en78cYwDNGwYHHhgHPy33DLrSBvJAw/E9OCDs40DWLEi2mGWLYsb8Tp2zDoiWV9mNs3daxxTN9UrAjMbAVwDlAE3u/vl1Zb/APgOsBpYBJzo7m+mGZOUhs8+iy6dL7wQpaIiHgizcmUs33lnOP74OPh/7WvQtm228WaiBBJAlbZt4Y47YM89YwgKjUfUvKSWCMysDLgO2A+oBJ43s8nuXniLyovAQHdfbmbfA34BNP4dNJKaVauiSufVV+G112I6eza89FKc7UPU9ffvH2MA7bNP9P7p0iXbuEvCq6/GdMcds40jMWhQNMRfdFE8u6ERRsiWRpJa1ZCZ7Qlc6O77J+/PAXD3y2pZf1fg1+6+V13bVdVQ6Vi9GpYsgXfeWXvvU/Uyd26c/VfZfPMY72fAgKhq2G23eN+qVVbfooQNHRrTp57KMop1fPppNMy/8UZUEW21VdYRSbGyqhrqBhTeFlkJ7F7H+icBj9S0wMzGAmMBttlmmw0K5ppr4Pzzo755k03qnrZtu3Za+LrYafV5ZWUbFHJq3OMg/sknUT7+GJYuhY8+WrcUzluyJB5asnjx2un7739+7PqysujS2aNHPOHrqKPihLaqqG65aWvVKqqFdt01HmTzwAPqrdUclESvITM7FhgI7FPTcne/CbgJ4opgQ/bRr1+MobJyZRz8app++GG8XrEi3hdON+bCqVWrdRNDVXKoXlq0qH1+ixbRV76wuH9+XtX8wgN9TWV9vk/r1tCpE3TtGlU2AwbEtOr95pvHgb9Hj2jIbVkSf1WSlp12ipv1TjstRnI9+eSsI5KNlea/7HygR8H77sm8dZjZMODHwD7u/klawXzta1E2hHvUdVdPDsVOq89buTKqSwrLmjWfn7dq1dr5a9asmxTM1r5u2fLz88vK4gqntlJ1BbTJJrDppjESZ/XSvn1MW7du2N+FNH3jxsUzjk8/PbqXbrdd1hHJxkgzETwP7GBmvYgEcDRwTOEKSbvAjcAId383xVg2itnag6b6rYvEycZtt8XAfscfH80YpVYFKsVLbYgJd18NjAMeA2YDE939FTO72MwOSVb7JdAO+KOZTTezyWnFI9LknHdelBLVo0c86vMf/4DLL69/fSlduqFMRDaYOxxzDEycCH/5S1QTSWmqq9eQBp0TKVXTp0cpYVXPLthpp3gGRGVl1hHJhlAiEClV48dHKXHt2sF990VniCOOiE4O0rQoEYjIRttpp2g8njo1HggkTYsSgYg0iMMPhx/8IBqQ77gj62hkfSgRiEiDufzyaDA++eToTSRNgxKBiDSYVq3gj3+Enj1h1KgYcFBKnwYDEClVP/tZ1hFskC98IR4mtPvuMZL2P/+pGzFLna4IRErVkCFRmqAddoD774+hx488Uj2JSp0SgUip+uc/ozRRQ4fCTTfFjWbHHx/jZUlpUtWQSKk699yYltDzCNbXCSfEsOVnnQWdO0ePIg1bXXqUCEQkVT/6USSDK66Ioct/8pOsI5LqlAhEJHW/+EU8zOjCC2MU37PPzjoiKaREICKpqxqTaNUqOOeceMbGj3+cdVRSRYlARBpFy5Zxx3GLFjG69urVqiYqFUoEIqXq6quzjqDBlZXB7bfH9MIL43nZl18eyUGyo0QgUqoGDMg6glSUlcGtt0J5Ofzyl7BgQbzXI1Gzo0QgUqoefzymw4ZlG0cKWrSA666D7t2jreCdd2Io6/bts44sn5QIRErVpZfGtBkmAogG5HPPha23jkHqhgyBP/8Ztt8+68jyRzVzIpKpMWPg0Ufh7bdh0CB45JGsI8ofJQIRydy++0JFBWy7LRx4YFwMffZZ1lHlhxKBiJSEXr1iaKXRo+H88yM5zJuXdVT5oEQgIiWjvBzuuisee1lRAf36wcSJWUfV/CkRiJSqG2+MkjNm0W4wfTrsuCMcdVQ85KayMuvImi8lApFSteOOUXJq++3h73+PG84eewz69o3RS9V20PCUCERK1QMPRMmxVq1iCOuZM2HPPeHUU2G33SIxSMNRIhApVVdeGUXYbrvoYjpxIixbBiNGwPDh8OKLWUfWPCgRiEiTYAZHHAGzZsGvfhWNybvtFs9FnjIl6+iaNiUCEWlSNtkExo+HOXPg4oujy+mQIfD1r0dNmtoQ1p8SgYg0SR07xv0Gb74JV10Fr74KhxwS1Ug/+1mMXyTFUSIQkSatXTs4/XSYOxcmTYIddoiB7Lp3h5Ej476Ejz7KOsrSZu6edQzrZeDAgV5RUZF1GCLpq7qttkePbONogl59NW5Ku+ceeOstaNs2hq446CA44ADYfPOsI2x8ZjbN3QfWuCzNRGBmI4BrgDLgZne/vNryTYA7gC8DS4Cj3H1uXdtUIhCRYq1ZEw3J99wD998fA9uZxeB2I0fC0KEweHAkiuYuk0RgZmXAa8B+QCXwPDDa3WcVrHMK0M/dv2tmRwOj3P2ourarRCC58Yc/xPSoOv8lpEju0d30oYeiPPdczGvdOhLD3nvDHntET6Ru3SJhNCdZJYI9gQvdff/k/TkA7n5ZwTqPJetMMbOWwEKgq9cRlBKB5MbQoTF96qkso2i23nsPnn027l5+5pnojlrV46hrV9h110gKu+yy9ibvzTbLNuaNUVciSPPBNN2AwrEDK4Hda1vH3Veb2YdAZ2Bx4UpmNhYYC7DNNtukFa+I5EinTnEPwsEHx/uPP4aXXoIXXojy4otwxRWwevXaz2y1VSSE3r1hm22i9OgR027domtrU9QknlDm7jcBN0FcEWQcjog0Q5tuGvcjDBmydt4nn8Drr8Nrr0UDdFW5/35YvPjz29hiC9hyy7ii2Hzzdaddu0byad8eOnSI0r49tCyBo3CaIcwHCrs7dE/m1bROZVI11IFoNBYRydwmm8DOO0epbvnyGBF13rzomfTWW/H63XejzJkDixbV33W1vHxtUujQIRJSeXmUtm3Xvi4vj/skBg1q+O+ZZiJ4HtjBzHoRB/yjgWOqrTMZOB6YAhwOPFlX+4CISKkoL48qot69615v5cpICIsWwfvvw4cfRlm6tObXK1bAwoWRaKqXHj2aWCJI6vzHAY8R3UdvdfdXzOxioMLdJwO3AHea2evAe0SyEBGIu6OkyWvTJg7gG3s7iHuUNKRaO+XuDwMPV5t3QcHrlcARacYg0mR16ZJ1BFJCzNLr0qohJkRK1e23RxFJmRKBSKlSIpBGokQgIpJzSgQiIjmnRCAiknNKBCIiOVcCNzeLSI0efrj+dUQagBKBSKkqL886AskJVQ2JlKrf/CaKSMqUCERK1cSJUURSpkQgIpJzSgQiIjmnRCAiknNKBCIiOZfaw+vTYmaLgDezjqNAF6o9Y7nElHp8UPoxlnp8UPoxlnp80Pxj3Nbdu9a0oMklglJjZhXuPjDrOGpT6vFB6cdY6vFB6cdY6vFBvmNU1ZCISM4pEYiI5JwSwca7KesA6lHq8UHpx1jq8UHpx1jq8UGOY1QbgYhIzumKQEQk55QIRERyTomggZjZGWbmZtYl61iqM7Nfmtn/mdkMM/uTmXXMOiYAMxthZq+a2etmdnbW8VRnZj3M7G9mNsvMXjGz07KOqSZmVmZmL5rZg1nHUhMz62hmk5K/wdlmtmfWMVVnZqcnv+OZZnaPmbXJOJ5bzexdM5tZMK+Tmf3VzP6dTL/QUPtTImgAZtYDGA68lXUstfgrsIu79wNeA87JOB7MrAy4DjgA6AuMNrO+2Ub1OauBM9y9L7AH8L8lGCPAacDsrIOowzXAo+6+E9CfEovVzLoBpwID3X0XoAw4OtuouB0YUW3e2cAT7r4D8ETyvkEoETSMXwE/Akqy5d3d/+Luq5O3U4HuWcaTGAy87u5z3H0VcC9waMYxrcPd33b3F5LXHxEHsG7ZRrUuM+sOHAjcnHUsNTGzDsDewC0A7r7K3T/INKiatQTamllLoBxYkGUw7v4M8F612YcCv0te/w74RkPtT4lgI5nZocB8d38p61iKdCLwSNZBEAfUeQXvKymxg2whM+sJ7Ar8K+NQqruaOAlZk3EctekFLAJuS6qvbjazTbMOqpC7zweuIK7o3wY+dPe/ZBtVjbZw97eT1wuBLRpqw0oERTCzx5O6w+rlUOBc4IISj7FqnR8T1R2/zy7SpsfM2gH3AePdfWnW8VQxs4OAd919Wtax1KElsBtwvbvvCnxMA1ZpNISkrv1QImltDWxqZsdmG1XdPPr9N1gNhJ5ZXAR3H1bTfDP7EvHH85KZQVS5vGBmg919YSOGWGuMVcxsDHAQsK+Xxs0j84EeBe+7J/NKipm1IpLA7939/qzjqWYv4BAzGwm0Adqb2V3uXkoHsUqg0t2rrqQmUWKJABgGvOHuiwDM7H5gCHBXplF93jtmtpW7v21mWwHvNtSGdUWwEdz9ZXff3N17untP4o9+t8ZOAvUxsxFE9cEh7r4863gSzwM7mFkvM2tNNM5NzjimdVhk91uA2e5+VdbxVOfu57h79+Rv72jgyRJLAiT/C/PMbMdk1r7ArAxDqslbwB5mVp78zvelxBq0E5OB45PXxwP/r6E2rCuCfPg1sAnw1+TKZaq7fzfLgNx9tZmNAx4jemnc6u6vZBlTDfYCjgNeNrPpybxz3f3h7EJqkr4P/D5J+HOAEzKOZx3u/i8zmwS8QFSdvkjGw02Y2T3AUKCLmVUCPwEuByaa2UnEUPxHNtj+SqOWQEREsqKqIRGRnFMiEBHJOSUCEZGcUyIQEck5JQIRkZxTIpBcMrNlRa43ycy2K3g/IBlldkS19erdXrH7LFj/IDO7eH0+I7IhlAhEamFmOwNl7j6nYPZo4B/JNG0PAQebWXkj7EtyTIlAcs3MhprZUwXj5f8+ubsU4FsU3L2ZzD8CGAPsV9OY9cn2njGzh5JnLdxgZi0Klv/UzF4ys6lmtkUy72Az+1cyKNvjVfOToUCeIoYGEUmNEoFIjCo6nnguwnbEHcUk08IB3YYQY9L8hzhAH1jL9gYTd9P2Bb4IfDOZvylxV3d/4Bng5GT+P4A9kkHZ7iWGA6lSAXx1A7+XSFGUCETgOXevdPc1wHSgZzJ/K2II5SqjiQM1ybS26qHnkucsfAbcA3wlmb8KqHqK2LSC/XQHHjOzl4EfAjsXbOtdYkRMkdQoEYjAJwWvP2PtGFwriFE9q56odhhwgZnNBa4FRpjZZjVsr/q4LVXvPy0Y+bVwP9cCv3b3LwH/U7XPRJskDpHUKBGI1G42sH3yel9ghrv3SEab3ZYYnnpUDZ8bnIyq2gI4iqj6qUsH1g7BfXy1Zb2BmYikSIlApHYPESNAQlQD/ana8vuouXroeWLE19nAGzV8rroLgT+a2TRgcbVlX0viEEmNRh8VqYWZtQX+BuyV1PcX85mhwJnuvtE9fZLeQ3e7+74buy2RuuiKQKQW7r6CGAc+q2cpbwOckdG+JUd0RSAiknO6IhARyTklAhGRnFMiEBHJOSUCEZGcUyIQEcm5/w/a94tOrCJRHAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha for Ridge=33.16019248594952, best OS R_2=0.4081703394189098\n"
     ]
    }
   ],
   "source": [
    "alphas = np.exp(np.linspace(-5,10,10000)) #exponential range of possible alphas\n",
    "[alpha_optimal, best_R2, best_coefs]=Regularization_fit_alpha(1,dataTrain.iloc[:,1:],dataTrain.Y,dataValid.iloc[:,1:],dataValid.Y,alphas,Graph=True,logl=True)\n",
    "print('Optimal alpha for Ridge={0}, best OS R_2={1}'.format(alpha_optimal,best_R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha for Lasso=0.21878801954843707, best OS R_2=0.40797693181670114\n"
     ]
    }
   ],
   "source": [
    "alphas = np.exp(np.linspace(-5,10,10000)) #exponential range of possible alphas\n",
    "[alpha_optimal, best_R2, best_coefs]=Regularization_fit_alpha(2,dataTrain.iloc[:,1:],dataTrain.Y,dataValid.iloc[:,1:],dataValid.Y,alphas,Graph=True,logl=True)\n",
    "print('Optimal alpha for Lasso={0}, best OS R_2={1}'.format(alpha_optimal,best_R2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now consider the test sample performanc of the best Lasso model and its coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IS R-squared of Lasso is: 0.6798581736149645\n",
      "The OS R-squared of Lasso is: 0.4519368356541291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.41519791, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.72985833, 0.68059909, 0.19852351, 0.        , 0.        ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lasso=linear_model.Lasso(fit_intercept=True, alpha=alpha_optimal)\n",
    "\n",
    "Lasso.fit(dataTrain.iloc[:,1:],dataTrain.Y)\n",
    "# In sample:\n",
    "YPred_IS=Lasso.predict(dataTrain.iloc[:,1:])\n",
    "print(\"The IS R-squared of Lasso is: {0}\".format(r2_score(dataTrain.Y,YPred_IS)))\n",
    "#Out of sample\n",
    "YPred_OS=Lasso.predict(dataTest.iloc[:,1:])\n",
    "print(\"The OS R-squared of Lasso is: {0}\".format(r2_score(dataTest.Y,YPred_OS)))\n",
    "Lasso.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Homework\n",
    "Recall the polynomial regression from previous class and try lasso for degree selection.\n",
    "\n",
    "First recreate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-10.0</td>\n",
       "      <td>62.503031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9.5</td>\n",
       "      <td>51.410761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9.0</td>\n",
       "      <td>44.605198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-8.5</td>\n",
       "      <td>24.739247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-8.0</td>\n",
       "      <td>22.200164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      x          y\n",
       "0 -10.0  62.503031\n",
       "1  -9.5  51.410761\n",
       "2  -9.0  44.605198\n",
       "3  -8.5  24.739247\n",
       "4  -8.0  22.200164"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#generate data and put it in the dataframe\n",
    "np.random.seed(2018)\n",
    "x=np.arange(-10,20,0.5)\n",
    "y=x**4/100+x**3/20+x**2/3+2*x+np.random.normal(loc=0,scale=3,size=60)\n",
    "data2=pd.DataFrame({'x':x,'y':y}) #create a dataframe\n",
    "#slice the data in three pieces (we'll talk about those later)\n",
    "data2.head() #for now let's stick with this first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.reindex(['y','x'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62.503031</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>-1000.000</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>-100000.00000</td>\n",
       "      <td>1000000.000000</td>\n",
       "      <td>-1.000000e+07</td>\n",
       "      <td>1.000000e+08</td>\n",
       "      <td>-1.000000e+09</td>\n",
       "      <td>1.000000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51.410761</td>\n",
       "      <td>-9.5</td>\n",
       "      <td>90.25</td>\n",
       "      <td>-857.375</td>\n",
       "      <td>8145.0625</td>\n",
       "      <td>-77378.09375</td>\n",
       "      <td>735091.890625</td>\n",
       "      <td>-6.983373e+06</td>\n",
       "      <td>6.634204e+07</td>\n",
       "      <td>-6.302494e+08</td>\n",
       "      <td>5.987369e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.605198</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>81.00</td>\n",
       "      <td>-729.000</td>\n",
       "      <td>6561.0000</td>\n",
       "      <td>-59049.00000</td>\n",
       "      <td>531441.000000</td>\n",
       "      <td>-4.782969e+06</td>\n",
       "      <td>4.304672e+07</td>\n",
       "      <td>-3.874205e+08</td>\n",
       "      <td>3.486784e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.739247</td>\n",
       "      <td>-8.5</td>\n",
       "      <td>72.25</td>\n",
       "      <td>-614.125</td>\n",
       "      <td>5220.0625</td>\n",
       "      <td>-44370.53125</td>\n",
       "      <td>377149.515625</td>\n",
       "      <td>-3.205771e+06</td>\n",
       "      <td>2.724905e+07</td>\n",
       "      <td>-2.316169e+08</td>\n",
       "      <td>1.968744e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22.200164</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>64.00</td>\n",
       "      <td>-512.000</td>\n",
       "      <td>4096.0000</td>\n",
       "      <td>-32768.00000</td>\n",
       "      <td>262144.000000</td>\n",
       "      <td>-2.097152e+06</td>\n",
       "      <td>1.677722e+07</td>\n",
       "      <td>-1.342177e+08</td>\n",
       "      <td>1.073742e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           y     x      x2        x3          x4            x5  \\\n",
       "0  62.503031 -10.0  100.00 -1000.000  10000.0000 -100000.00000   \n",
       "1  51.410761  -9.5   90.25  -857.375   8145.0625  -77378.09375   \n",
       "2  44.605198  -9.0   81.00  -729.000   6561.0000  -59049.00000   \n",
       "3  24.739247  -8.5   72.25  -614.125   5220.0625  -44370.53125   \n",
       "4  22.200164  -8.0   64.00  -512.000   4096.0000  -32768.00000   \n",
       "\n",
       "               x6            x7            x8            x9           x10  \n",
       "0  1000000.000000 -1.000000e+07  1.000000e+08 -1.000000e+09  1.000000e+10  \n",
       "1   735091.890625 -6.983373e+06  6.634204e+07 -6.302494e+08  5.987369e+09  \n",
       "2   531441.000000 -4.782969e+06  4.304672e+07 -3.874205e+08  3.486784e+09  \n",
       "3   377149.515625 -3.205771e+06  2.724905e+07 -2.316169e+08  1.968744e+09  \n",
       "4   262144.000000 -2.097152e+06  1.677722e+07 -1.342177e+08  1.073742e+09  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M=10\n",
    "for p in range(2,M+1): #add more powers of x up to 10'th\n",
    "    data2['x%d'%p]=data2.x**p\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2_Valid=data2.loc[40:49]\n",
    "data2_Test=data2.loc[50:59]\n",
    "data2_Train=data2.loc[0:39] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1 First try the model with a single alpha = 1. Assess the model performance over the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 Next perform hyperparameter selection over the validation set. Assess the resulting model.\n",
    "Find the best alpha from np.linspace(-10,10,200) maximizing the validation R2. Assess the performance of the model with the best alpha over the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 Standardize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you noticed the problem with the models above this could be related to the scale of the data.\n",
    "Regularization applies the same magnitude criteria to all the coefficients. While in case of a polynomial regression, regressors (powers of x) have totally different scale and so do the coefficients. \n",
    "\n",
    "So in order to perform a fair normalization we need to bring regressors on the same scale (normalize) first before applying regularization. \n",
    "\n",
    "So in this task **standardize the data** subtracting the sample mean and dividing by the standard deviation.\n",
    "\n",
    "Important note: standardization involves learning the parameters - mean and standard deviation from the data. As we're only supposed to learn from the training data but never from validation or test samples the mean and standard deviation for the standardization need to be learned from the training data set; and then the standardizing transformation will be applied to validation and test sets as well using the same parameters. This way the resulting validation and test samples won't get exactly standardized, but the transform (which is part of the model) will use only the parameters learned from the training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Next perform hyperparameter selection over the validation set using the standardized data. Assess the resulting model\n",
    "Find the best alpha from np.linspace(-10,10,200) maximizing the validation R2 over the standardized data. Assess the performance of the model with the best alpha over the test set. Report the resulting coefficients. What is the degree of the resulting polynomial model and is it consistent with the original model used to generate the data above?\n",
    "\n",
    "Note: the model's coefficient's apply to standardized data and this way may not be consistent with the original model. To bring the model on the original scale one may need to apply an inverse transform involving the standardization parameters, learned over the training data (not required in the task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
